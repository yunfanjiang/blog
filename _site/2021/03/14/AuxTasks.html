<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>On Auxiliary Tasks in Deep Reinforcement Learning | Yunfan Jiang’s Blog</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="On Auxiliary Tasks in Deep Reinforcement Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In the world of reinforcement learning (RL), a new-born agent sometimes struggles at reaching goals that we wish it could achieve. RL practicers always augment the learning process of the agent with auxiliary tasks, which help the agent to learn much faster, more robustly, and ideally perform better. In this post, we will summarize those auxiliary tasks used in deep RL, share ideas behind intuitions, and hopefully inspire you in related work." />
<meta property="og:description" content="In the world of reinforcement learning (RL), a new-born agent sometimes struggles at reaching goals that we wish it could achieve. RL practicers always augment the learning process of the agent with auxiliary tasks, which help the agent to learn much faster, more robustly, and ideally perform better. In this post, we will summarize those auxiliary tasks used in deep RL, share ideas behind intuitions, and hopefully inspire you in related work." />
<link rel="canonical" href="http://localhost:4000/2021/03/14/AuxTasks.html" />
<meta property="og:url" content="http://localhost:4000/2021/03/14/AuxTasks.html" />
<meta property="og:site_name" content="Yunfan Jiang’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-03-14T00:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="On Auxiliary Tasks in Deep Reinforcement Learning" />
<script type="application/ld+json">
{"description":"In the world of reinforcement learning (RL), a new-born agent sometimes struggles at reaching goals that we wish it could achieve. RL practicers always augment the learning process of the agent with auxiliary tasks, which help the agent to learn much faster, more robustly, and ideally perform better. In this post, we will summarize those auxiliary tasks used in deep RL, share ideas behind intuitions, and hopefully inspire you in related work.","headline":"On Auxiliary Tasks in Deep Reinforcement Learning","dateModified":"2021-03-14T00:00:00+08:00","datePublished":"2021-03-14T00:00:00+08:00","url":"http://localhost:4000/2021/03/14/AuxTasks.html","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2021/03/14/AuxTasks.html"},"@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Yunfan Jiang&apos;s Blog" /><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  </script>
  
  <script type="text/javascript"
     src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js">
  </script>
  
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Yunfan Jiang&#39;s Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class='page-link' href='http://web.stanford.edu/~yjiang05/' target='_blank'>About</a>
            <a class='page-link' href='https://github.com/yjiang05/yjiang05.github.io' target='_blank'>Site Code</a>
            <!-- <a class="page-link" href="/about/">About</a> --></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">On Auxiliary Tasks in Deep Reinforcement Learning</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2021-03-14T00:00:00+08:00" itemprop="datePublished">Mar 14, 2021
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>In the world of reinforcement learning (RL), a new-born agent sometimes struggles at reaching goals that we wish it could achieve. RL practicers always augment the learning process of the agent with auxiliary tasks, which help the agent to learn much faster, more robustly, and ideally perform better. In this post, we will summarize those auxiliary tasks used in deep RL, share ideas behind intuitions, and hopefully inspire you in related work.</p>

<h2 id="table-of-contents">Table of Contents</h2>

<ul>
  <li>What are Auxiliary Tasks?</li>
  <li>Enemy Detection</li>
  <li>Depth Prediction</li>
  <li>Loop Closure Classification</li>
  <li>Pixel Control</li>
  <li>Feature Control</li>
  <li>Reward Prediction</li>
  <li>Forward &amp; Inverse Dynamics</li>
  <li>CPC|A</li>
  <li>Auxiliary Predictive Modeling Tasks</li>
  <li>Auxiliary Policies</li>
  <li>Summary</li>
</ul>

<h2 id="what-are-auxiliary-tasks">What are Auxiliary Tasks</h2>

<p>It is uneasy to define <em>Auxiliary Tasks</em> in a nutshell because they are fairly different from each other. Some of them are task-specific, e.g., Enemy Detection, while some are generally applicably, e.g., Reward Prediction. Some of them are used to assist to learn better representations, e.g., Loop Closure Classification, while some are used to model relations between future and history context, e.g., CPC|A. Most of them can be implemented by optimizing explicit objective functions, while some can only be done by optimizing surrogate losses instead because the original objectives might be intractable, e.g., Auxiliary Predictive Modeling Tasks with <a href="https://yjiang05.github.io/ELBO/" target="_blank">ELBO</a>. Some even builds auxiliary policies with information asymmetry against the main policy and uses them to regularize the main policy.</p>

<p>However, we can still find some common properties those auxiliary tasks possess:</p>

<ul>
  <li>They introduce additional losses, which are optimized jointly with the main RL objective, i.e., \(\mathbb{E}_\pi\left[\Sigma_t \gamma^t r\right(s_t, a_t\left) \right]\) or \(\mathbb{E}_\pi\left[\Sigma_t \gamma^t r\right(s_t, a_t\left)  + \alpha \mathcal{H}\left( \pi\right)\right]\) in maximum entropy RL.</li>
  <li>They share parameters with the main neural network, introducing extra signals to update parameters and allowing, for example, better learning of representations and regularization on the main policy.</li>
  <li>They can be safely removed during inference, which adds no cost when your agent is deployed.</li>
  <li>They are self-supervised, meaning that you do not need to manually construct labels for training those auxiliary tasks.</li>
</ul>

<p>So far, you may have obtained a brief understanding about auxiliary tasks. Now let’s go through them one by one. I summarized several auxiliary tasks and listed them in chronological order (almost) by their appearance time. Hopefully you will see how they evolved and became much more important for training RL agents.</p>

<h2 id="enemy-detection">Enemy Detection</h2>

<p>The very first employment of auxiliary tasks in RL can be traced back to 2016 in the paper <a href="https://arxiv.org/abs/1609.05521" target="_blank"><em>Playing FPS Games with Deep Reinforcement Learning</em></a>, where authors augmented their models to exploit game features. Concretely, they trained a <a href="https://arxiv.org/abs/1507.06527" target="_blank">DRQN</a> to play the game <a href="https://github.com/mwydmuch/ViZDoom" taget="_blank">Doom</a> by asking their models whether an enemy exists in the current frame. See the figure below for an illustration. This screenshot comes from <a href="https://arxiv.org/abs/1809.03470" target="_blank">here</a>.</p>

<p class="center"><img src="/assets/img/posts/2021-03-14-AuxTasks/vizdoom_screenshot.png" alt="image" style="zoom:50%;" /></p>

<p>The motivation is that authors hypothesized that the agents could not accurately detect enemies. To help agents to better distinguish enemies appearing in the frame, at the output of CNN, they cascaded an MLP to answer the question about the appearance of enemies. Concretely, the MLP outputs the probability that enemies appear in the frame, which is then used to calculate a binary cross-entropy loss against binary labels provided by the game engine, i.e., \(\mathcal{L}_{EnemyDetection} = y_t\cdot \log P_t + (1 - y_t) \cdot \log (1 - P_t)\). Pseudo-code is as below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">loss_enemy_detection</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">label</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">out_p</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">label</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">out_p</span><span class="p">))</span>
</code></pre></div></div>

<p>Since gradients from this auxiliary loss can flow back to CNN, parameters in the CNN can be optimized toward more accurately capturing enemies, and hence provides the recurrent unit with information about the presence or absence of enemies, their locations, their distances, and so on. Authors reported doubled performance in terms of Kill-to-Death ratio of agents with enemy detection against those without.</p>

<p>As the first auxiliary task reported in RL literature, enemy detection is fairly intuitive and straightforward. In the original paper, authors claim that you can build an MLP with <em>k</em> outputs to detect <em>k</em> game features, providing more possibility to improve the learning of representations. Notwithstanding, drawbacks are obvious. First, this design is quite task-specific and introduces extra requirements on the environment, i.e., your simulator should provide relevant information you need. And also, detecting features in the current frame does not take account of temporal information. In my opinion, the auxiliary task of <em>Reward Prediction</em> that we will discuss later is a good alternative to Enemy Detection.</p>

<h2 id="depth-prediction">Depth Prediction</h2>

<p>The auxiliary task of <em>Depth Prediction</em> is introduced in paper <a href="https://arxiv.org/abs/1611.03673" target="_blank"><em>Learning to Navigate in Complex Environments</em></a>. It was proposed at almost the same time as Enemy Detection. In that paper, authors built an agent for navigation in complex 3D mazes, <a href="https://github.com/deepmind/lab" target="_blank">DeepMind Lab</a>, and claimed higher data efficiency and task performance by jointly optimizing the RL objective and auxiliary losses of Depth Prediction and Loop Closure Classification (which we will discuss in the next section).</p>

<p>Depth map reveals great information about 3D objects and 3D scenes. Grounding on the <a href="https://arxiv.org/abs/1406.2283" target="_blank">progress</a> in CV, the auxiliary task of depth prediction asks agents to predict the depth information in the central field of view from multimodal sensory inputs, e.g., RGB observations, agent-relative velocity, etc. The depth predictor takes input either from convolutional layers or the LSTM. In the first case, the prediction only conditions on the current RGB observation, while in the later case, the prediction conditions on the history of observations (RGB observation and agent-relative velocity), action, and reward, which is equivalent to condition the prediction on the <em>belief state</em>. From the view of <a href="https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process" target="_blank">POMDP</a>, an agent cannot obtain the full state of the environment due to its partial observability \(O(\cdot \vert s,a)\). Therefore, to make decisions under uncertainty, the agent has to repeatedly ask itself the question “what is the real state I am currently at?”. Formally, we say the agent needs to maintain a distribution, \(P_b(\cdot \vert h_t)\), over true states given its history \(h_t = \{o_0, a_0, o_1, a_1, \ldots a_{t-1}, o_t\}\) and call this distribution <em>belief state</em>. You can understand this by thinking the example of finding some location in downtown you’ve never visited without Google Map. You continuously update your guess about your current location by integrating what you’ve seen, e.g., buildings, landmarks, etc, and where you’ve moved. Now let’s get back to depth prediction. Intuitively, predicting the depth information given belief state is much easier than that solely given the current observation. What’s more, prediction conditioned on belief state also contributes gradients to the recurrent unit, helping to better model the representation of belief state and hence resulting in, ideally, higher task performance. Experimental results in the paper support this claim. Authors compared task performance of depth prediction conditioned on current RGB observation and belief state across four maps, i.e., small/large static/random mazes. I personally order these four maps by how partially observable they are: small static maze &lt; large static maze &lt; small random maze &lt; large random maze. Although agents trained using depth prediction conditioned on belief state outperform those trained using depth prediction conditioned on current RGB observation in all four maps, the headroom increases as the map becomes more partially observable, i.e., 1.7% in small static maze, 3.6% in large static maze, 20.0% in small random maze, and 28.8% in large random maze. All agents augmented with depth prediction outperform their vanilla counterparts.</p>

<p>After feeling the effect of depth prediction, let’s see how it is implemented. First of all, considering the computational budget, the agent is asked to predict a cropped and subsampled depth map of size \(4 \times 16\), instead of the full-size map of \(84 \times 84\). An example of the prediction can be seen below. This screenshot is taken from the <a href="https://www.youtube.com/watch?v=JL8F82qUG-Q" target="_blank">video</a> released with the paper.</p>

<p class="center"><img src="/assets/img/posts/2021-03-14-AuxTasks/depth_prediction.png" alt="image" style="zoom:50%;" /></p>

<p>The prediction loss can be calculated in either a regression manner or a classification way. In the regression way, the loss for depth prediction at time step \(t\) can be computed as \(\mathcal{L}=\frac{1}{2} \Vert \hat{d}_t - d_t \Vert^2_2\). In the classification way, the depth value at each pixel is quantized into 8 intervals, which is subsequently used as labels with the predictor’s softmax outputs to calculate a cross entropy loss. Specifically, authors empirically used the non-uniformally quantization <code class="language-plaintext highlighter-rouge">[0, 189, 214, 226, 234, 240, 245, 249, 255]</code>, which makes agents to pay more attention to distant objects. Superiority of prediction as classification against that as regression is reported. It is reasonable because prediction as classification softens the loss space to some extent and hence makes the learning easier.</p>

<p>Though the auxiliary task of depth prediction significantly speeds up learning navigation, <a href="https://arxiv.org/abs/2007.04561" target="_blank">Ye <em>et al.</em></a> argued that it is not generally applicable in both simulation and the real world and proposed to use more self-supervised tasks. Nevertheless, in my opinion, the idea of helping to represent belief states by predicting features of 3D objects/scenes from other modalities is much more valuable.</p>

<h2 id="loop-closure-classification">Loop Closure Classification</h2>

<p>The auxiliary task of <em>Loop Closure Classification</em> is proposed with Depth Prediction in the same paper. The motivation is for the sake of more efficient exploration and spatial reasoning. Concretely, given locations up to time <em>t</em> in an episode ${p_0, p_1, \ldots , p_t}$, a loop closure label at time step <em>t</em> is true if $\vert p_t - p_j \vert \leq \eta_1$ where $j &lt; t$ and $\vert p_t - p_{anchor}\vert \leq \eta_2$ where $p_{anchor}$ is an anchor point being far from $p_t$ to avoid trivial loops. An MLP works as the loop closure classifier conditioned on the hidden representation of the LSTM. This task is optimized by minimizing a binary cross-entropy loss between predictions and true labels.</p>

<p>The following image is taken from Figure 4 in the original paper and shows a trajectory. The grey square represents the starting position. Grey dots show paths of the agent. Blue dots represent true positive prediction (a loop closure happens and the classifier correctly predicts that). Red dots represent false positive prediction. Green dots show false negative prediction. It is interesting to note that the segment where the agent makes false positive predictions. The agent might reckon that it is in a loop only because it is close to the beginning. However, that segment is not in a loop during the exploration. This is one reason that I argue that this auxiliary task is a bit unnecessary and even deleterious to the task performance. Experimental results also support this argument that the task performance is not monotonically improved by augmenting with loop closure classification.</p>

<p class="center"><img src="/assets/img/posts/2021-03-14-AuxTasks/loop_closure_prediction_example.png" alt="image" style="zoom:50%;" /></p>

<h2 id="pixel-control">Pixel Control</h2>

<p>Auxiliary tasks of <em>Pixel Control</em>, <em>Feature Control</em>, and <em>Reward Prediction</em> are proposed in the <a href="https://arxiv.org/abs/1611.05397" target="_blank"><em>UNREAL</em> agent</a>. Motivated by the hypothesis that an agent can achieve any goals if it can control its future, authors developed pixel control and feature control as auxiliary control tasks. To enable the agent predict its future, authors also proposed auxiliary reward tasks. All these tasks are <em>unsupervised</em>. Training them only relies on signals provided by pseudo reward functions, alleviating the reward sparsity generally encountered. Furthermore, policies for auxiliary tasks share parameters with the base policy, asking the agent to balance the main task and auxiliary tasks. We will talk about pixel control in this section by discussing its definition, motivation, and implementation. I also attached some Python codes for those who wish to implement it by themselves.</p>

<p>An auxiliary control task <em>c</em> is defined by a reward function \(r^{(c)}: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\). Thinking a baby crawling around in the garden, what he/she sees changes rapidly and dramatically to satisfy his/her curiosity. Likewise, the auxiliary task of pixel control is proposed to maximize the pixel changing in its perceptual stream. Concretely, using the averaged absolute pixel change between successive RGB observations as the reward signal, pixel control is optimized as an <em>n</em>-step Q-learning:</p>

\[\mathcal{L}_{PC}^{(c)}= \mathbb{E}\left[ \left( R_{t:t+n} + \gamma^n \max_{a'} Q^{(c)} \left( s', a', \theta^{-}\right) - Q^{(c)}\left(s, a, \theta \right)\right) ^ 2 \right],\]

<p>where \(\theta^-\) represents parameters of the target Q function and the optimization is taken with respect to \(\theta\). The policy network for pixel control \(\pi^{(c)}\) takes the output from LSTM as input, enabling its access to the history of observations and rewards. The number of heads is equal to the number of action heads and each head outputs an $N_{act} \times n \times n$ Q-estimation.</p>

<p>In practice, visual observations are always cropped and subsampled. With a conventional size of $84 \times 84$ for image inputs, they are first cropped to the central $80 \times 80$ area and then subsampled with a factor of 4, resulting in the size of $20 \times 20$. Pseudo reward is computed as the average absolute pixel change between successive image observations, which is illustrated in the following code block.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="c1"># each side is cropped by 2
</span><span class="n">CROP</span> <span class="o">=</span> <span class="mi">2</span>
<span class="c1"># subsample factor
</span><span class="n">SSF</span> <span class="o">=</span> <span class="mi">4</span>


<span class="k">def</span> <span class="nf">pixel_change_reward</span><span class="p">(</span><span class="n">observations</span><span class="p">):</span>
    <span class="s">"""
    Compute pixel change between successive RGB observations
    Args:
        observations (torch.Tensor): RGB observations of size (N, L + 1, C, H, W)
    Returns:
        pc_reward (torch.Tensor): Pseudo reward of size (N, L, H_crop_subsample, W_crop_subsample)
    """</span>
    <span class="c1"># crop obs
</span>    <span class="n">observations</span> <span class="o">=</span> <span class="n">observations</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="n">CROP</span><span class="p">:</span><span class="o">-</span><span class="n">CROP</span><span class="p">,</span> <span class="n">CROP</span><span class="p">:</span><span class="o">-</span><span class="n">CROP</span><span class="p">]</span>
    <span class="c1"># compute average absolute pixel change between successive images
</span>    <span class="c1"># (N, L + 1, C, H, W) -&gt; (N, L, H, W)
</span>    <span class="n">pixel_change</span> <span class="o">=</span> <span class="p">(</span><span class="n">observations</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">observations</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]).</span><span class="nb">abs</span><span class="p">().</span><span class="n">mean</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="c1"># subsample
</span>    <span class="c1"># (N, L, H, W) -&gt; (N, L, H / SSF, W / SSF)
</span>    <span class="n">pc_reward</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">avg_pool2d</span><span class="p">(</span><span class="n">pixel_change</span><span class="p">,</span> <span class="n">SSF</span><span class="p">,</span> <span class="n">SSF</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pc_reward</span>
</code></pre></div></div>

<p>A deconvolutional network, which exploits the spatial correlation, is used to produce $N_{act} \times 20 \times 20$ Q-estimation from the outputs of the LSTM. The <a href="https://arxiv.org/abs/1511.06581" target="_blank">dueling factorization</a> is used to learn the value function more efficiently. Specifically, the deconvolutional network consists of two steams with 1 and $N_{act}$ output channels, producing the state value and advantage, respectively. Finally, with the pseudo reward we just calculated, the auxiliary task of pixel control can be optimized by minimizing the <em>n</em>-step Q-learning loss. Here is a code snippet demonstrating this process.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>


<span class="k">class</span> <span class="nc">PixelControl</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="s">"""Compute the n-step Q learning loss for pixel control."""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="s">"""
        Args:
            model (nn.Module): your agent model with two attributes `pc_v` and `pc_adv`.
            gamma (float): discount factor
        """</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_mse_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MSELoss</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_calc_q</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lstm_out</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">done</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="s">"""
        Calculate Q values for pixel control.
        t-th done tells whether the t-th state is terminated
        Args:
            lstm_out (torch.Tensor): output from LSTM with shape (N, L + 1, hidden_size)
            done (torch.Tensor): (N, L + 1, 1)
        Returns:
            q_masked (torch.Tensor): q values with shape (N, L + 1, N_act, H, W)
        """</span>
        <span class="c1"># first we combine the first two axes
</span>        <span class="n">lstm_out</span> <span class="o">=</span> <span class="n">lstm_out</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">lstm_out</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="c1"># forward the v net
</span>        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_model</span><span class="p">.</span><span class="n">pc_v</span><span class="p">(</span><span class="n">lstm_out</span><span class="p">)</span>    <span class="c1"># (N * (L + 1), 1, 20, 20)
</span>        <span class="c1"># forward the advantage net
</span>        <span class="n">adv</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_model</span><span class="p">.</span><span class="n">pc_adv</span><span class="p">(</span><span class="n">lstm_out</span><span class="p">)</span>    <span class="c1"># (N * (L + 1), N_act, 20, 20)
</span>        <span class="c1"># now calculate q value with dueling factorization
</span>        <span class="n">n_act</span> <span class="o">=</span> <span class="n">adv</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">adv_mean</span> <span class="o">=</span> <span class="n">adv</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="n">repeat</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_act</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">v</span><span class="p">.</span><span class="n">repeat</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_act</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="n">adv</span> <span class="o">-</span> <span class="n">adv_mean</span>    <span class="c1"># (N * (L + 1), N_act, 20, 20)
</span>        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">done</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">done</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]))</span>    <span class="c1"># (N, L + 1, N_act, 20, 20)
</span>        <span class="c1"># now we need to mask q values for terminal states
</span>        <span class="n">done</span> <span class="o">=</span> <span class="n">done</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]])</span>
        <span class="n">q_masked</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">done</span><span class="p">)</span> <span class="o">*</span> <span class="n">q</span>
        <span class="k">return</span> <span class="n">q_masked</span>

    <span class="k">def</span> <span class="nf">calc_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">lstm_out</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">done</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="s">"""
        Calculate n-step Q loss for pixel control
        Args:
            obs (torch.Tensor): RGB observations of size (N, L + 1, C, H, W)
            lstm_out (torch.Tensor): output from LSTM with shape (N, L + 1, hidden_size)
            action (torch.Tensor): (N, L, 1)
            done (torch.Tensor): (N, L + 1, 1)
        Returns:
            Loss
        """</span>
        <span class="c1"># compute the pseudo reward
</span>        <span class="n">r</span> <span class="o">=</span> <span class="n">pixel_change_reward</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>    <span class="c1"># (N, L, H_crop_subsample, W_crop_subsample)
</span>        <span class="n">N</span><span class="p">,</span> <span class="n">L</span> <span class="o">=</span> <span class="n">r</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">r</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># mask r in terminal states
</span>        <span class="n">mask</span> <span class="o">=</span> <span class="n">done</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">r</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">mask</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">r</span>
        <span class="c1"># compute q
</span>        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_calc_q</span><span class="p">(</span><span class="n">lstm_out</span><span class="p">,</span> <span class="n">done</span><span class="p">)</span>    <span class="c1"># (N, L + 1, N_act, H, W)
</span>
        <span class="c1"># q estimations
</span>        <span class="c1"># we use a trick here to avoid loop
</span>        <span class="n">q_estimate</span> <span class="o">=</span> <span class="n">q</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">N</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">L</span><span class="p">]),</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">L</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">repeat</span><span class="p">([</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
            <span class="n">action</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">]</span>    <span class="c1"># (N, L, H, W)
</span>        <span class="c1"># calculate q targets
</span>        <span class="n">q_targets</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">last_max_q</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">q</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">).</span><span class="n">values</span>
        <span class="n">temp</span> <span class="o">=</span> <span class="n">last_max_q</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">L</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">temp</span> <span class="o">=</span> <span class="n">r</span><span class="p">[:,</span> <span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">_gamma</span> <span class="o">*</span> <span class="n">temp</span>
            <span class="n">q_targets</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp</span><span class="p">)</span>
        <span class="n">q_targets</span><span class="p">.</span><span class="n">reverse</span><span class="p">()</span>
        <span class="c1"># stop gradient for target
</span>        <span class="n">q_targets</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">(</span><span class="n">q_targets</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">detach</span><span class="p">()</span>    <span class="c1"># (N, L, H, W)
</span>
        <span class="c1"># finally calculate loss
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_mse_loss</span><span class="p">(</span><span class="n">q_estimate</span><span class="p">,</span> <span class="n">q_targets</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>
</code></pre></div></div>

<p>To find out how much the addition of pixel control helps the learning, let’s compare performances of a vanilla A3C agent, variants augmented with input reconstruction/input change prediction, and an A3C agent with pixel control. While the input reconstruction is optimized in a self-supervised manner with one head trying to reconstruct input images, the input change prediction is similar to pixel change instead that it only predicts the auxiliary reward. The A3C agent with input reconstruction learns faster than the vanilla A3C agent at the beginning. However, it is worse than the vanilla one in terms of the final performance. Signals from input reconstruction loss speeds up the learning of representation when training from scratch. However, the agent may struggle at reconstructing irrelevant features as the training continues, which instead limits its performance. Furthermore, the agent with input change prediction outperforms that with input reconstruction. My interpretation is that comparing with only the current observation, change between successive observations takes account some temporal information. Therefore, learning to predicting that is more beneficial for learning in such partially observable tasks. And hence, using a <em>n</em>-step Q learning loss to optimize controlling that information is much more beneficial due to longer temporal dependencies involved, which leads to the best performance achieved by the agent with pixel control.</p>

<p class="center"><img src="/assets/img/posts/2021-03-14-AuxTasks/pixel_control.png" alt="image" style="zoom:50%;" /></p>

<h2 id="feature-control">Feature Control</h2>

<p>The auxiliary task of <em>Feature Control</em> is similar to pixel control, except that the quantity being controlled, i.e., <em>feature</em>, is expressed in a way which is much easier to understand for the neural networks. Concretely, activated outputs from hidden units are used as such quantity. The pseudo auxiliary reward is thus computed as the absolute difference between successive outputs from hidden units. In contrast to sensorimotor inputs such as RGB images, outputs from intermediate layers in the neural network consist of more high-level and more task-relevant information extracted by the model. That’s why I said that this auxiliary control task is learnt in a more “model-friendly” way. To do so, we just need to replace the input to the previous function <code class="language-plaintext highlighter-rouge">pixel_change_reward</code> by, for example, outputs from the second convolutional layer of our CNN. Experiment shows that the A3C agent with feature control speeds up the learning from scratch, leading to higher data efficiency.</p>

<p class="center"><img src="/assets/img/posts/2021-03-14-AuxTasks/feature_control.png" alt="image" style="zoom:25%;" /></p>

<h2 id="reward-prediction">Reward Prediction</h2>

<p>A good agent should be able to recognize states leading to high reward and value. However, learning of such recognization is challenging in many environments due to reward sparsity. The auxiliary task of <em>Reward Prediction</em> with a specially designed experience replay buffer aims to mitigate this issue without introducing any bias into the agent’s policy. This auxiliary task asks the agent to predict the immediate reward given some history of observations. Since gradients from such supervised signal do not flow back to the policy network, it only help to shape the features and does not bias policy or value. To be specific, the agent is given a history of embedded observations \(H_{\tau}=\{f\left(o_{\tau - k}\right),f\left(o_{\tau - k+1}\right),\ldots,f\left(o_{\tau - 1}\right)\}\) and asked to predict the immediate reward \(r_\tau\) in the subsequent unseen frame. This is equivalent to an myopic version of TD(1), i.e., \(\gamma=0\):</p>

\[\begin{align}
\mathcal{L}_{RP} &amp;=\mathbb{E}_{H_\tau \sim \mathcal{H}} \left[\left( r_\tau + \gamma f_{RP}\left(H_{\tau+1}; \phi_{RP}\right) - f_{RP}\left(H_\tau;\phi_{RP}\right) \right)^2\right] \bigg\rvert_{\gamma=0} \\
&amp;=\mathbb{E}_{H_\tau \sim \mathcal{H}} \left[\left( r_\tau - f_{RP}\left(H_\tau;\phi_{RP}\right) \right)^2\right] ,
\end{align}\]

<p>where \(f_{RP}\left(\cdot; \phi_{RP}\right)\) is the network parameterized by\(\phi_{RP}\) predicting the reward and \(\mathcal{H}\) is a specially designed experience replay buffer for this auxiliary task, which I shall explain immediately. Alternatively, this auxiliary task can be trained by minimizing a cross entropy loss with three classes, i.e., zero reward, positive reward, and negative reward.</p>

<p>To overcome the reward sparsity, an experience replay buffer is intentionally split to be two parts, one for zero-reward histories and the other for rewarding histories. Samples are equally replayed from these two subsets such that \(P\left(r_\tau \neq 0\right) = 0.5\). This replay buffer is demonstrated in the figure below.</p>

<p class="center"><img src="/assets/img/posts/2021-03-14-AuxTasks/rp_buffer.png" alt="image" style="zoom:125%;" /></p>

<p>We use a length of 3 for history context in experiments and the following code snippet. To simplify the temporal dependencies and only ask the agent to focus on predicting the immediate reward, the computation is done in a feedforward manner. An MLP is used to predict the label from stacked 3 successive embedded features from CNN. Gradients from the cross entropy loss between predictions and ground truths thus help to shape and speed up the learning of features. I demonstrate this process in the code block below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">deque</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>


<span class="k">class</span> <span class="nc">Worker</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="s">"""A simple worker that interacts with env and generates samples"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">history_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">):</span>
        <span class="s">"""
        Args:
            history_len (int): length of history context for reward prediction
        """</span>
        <span class="c1"># we use a deque to cache history context
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">_cache</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">history_len</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_hist_len</span> <span class="o">=</span> <span class="n">history_len</span>

    <span class="k">def</span> <span class="nf">rollout</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># initialize an env
</span>        <span class="n">env</span> <span class="o">=</span> <span class="n">Env</span><span class="p">()</span>

        <span class="c1"># reset env
</span>        <span class="n">obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="c1"># add obs to cache
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">_cache</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>

        <span class="c1"># start rollout
</span>        <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_act</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
            <span class="n">obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="c1"># add obs to cache
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">_cache</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
            <span class="c1"># push history context
</span>            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">_cache</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_hist_len</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">history_context</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span>
                    <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">_cache</span><span class="p">.</span><span class="n">popleft</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">_hist_len</span><span class="p">)]</span>
                <span class="p">)</span>
                <span class="c1"># now push history context to corresponding replay buffer according to the immediate reward
</span>                <span class="k">if</span> <span class="n">reward</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="bp">self</span><span class="p">.</span><span class="n">_push_to_non_rewarding_replay</span><span class="p">(</span>
                        <span class="p">{</span>
                            <span class="s">'history_context'</span><span class="p">:</span> <span class="n">history_context</span><span class="p">,</span>
                            <span class="s">'rp_label'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
                        <span class="p">}</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="p">.</span><span class="n">_push_to_rewarding_replay</span><span class="p">(</span>
                        <span class="p">{</span>
                            <span class="s">'history_context'</span><span class="p">:</span> <span class="n">history_context</span><span class="p">,</span>
                            <span class="s">'rp_label'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span> <span class="k">if</span> <span class="n">reward</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
                        <span class="p">}</span>
                    <span class="p">)</span>


<span class="k">class</span> <span class="nc">RewardPrediction</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="s">"""Compute the cross entropy loss for reward prediction."""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="s">"""
        Args:
            model (nn.Module): your agent model with attribute `rp_mlp` and `cnn`
        """</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_model</span> <span class="o">=</span> <span class="n">model</span>

    <span class="k">def</span> <span class="nf">calc_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">history</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">label</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="s">"""
        Args:
            history (torch.Tensor): history context of RGB observations with shape (N, hist_len, C, H, W)
            label (torch.Tensor): labels for reward prediction with shape (N, 3)
        """</span>
        <span class="c1"># first we combine the first two axes
</span>        <span class="n">N</span><span class="p">,</span> <span class="n">HIST_LEN</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">history</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">history</span> <span class="o">=</span> <span class="n">history</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">N</span> <span class="o">*</span> <span class="n">HIST_LEN</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">))</span>

        <span class="c1"># we call the CNN to process these history contexts
</span>        <span class="c1"># we assume your CNN outputs flattened features
</span>        <span class="n">extracted_history</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_model</span><span class="p">.</span><span class="n">cnn</span><span class="p">(</span><span class="n">history</span><span class="p">)</span>    <span class="c1"># (N * HIST_LEN, -1)
</span>
        <span class="c1"># we recover the dimension for history length
</span>        <span class="n">extracted_history</span> <span class="o">=</span> <span class="n">extracted_history</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">HIST_LEN</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>    <span class="c1"># (N, HIST_LEN, -1)
</span>        <span class="c1"># we stack successive observations
</span>        <span class="n">extracted_history</span> <span class="o">=</span> <span class="n">extracted_history</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>    <span class="c1"># (N, -1)
</span>
        <span class="c1"># now we feed those features to the reward prediction MLP to get logits
</span>        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_model</span><span class="p">.</span><span class="n">rp_mlp</span><span class="p">(</span><span class="n">extracted_history</span><span class="p">)</span>    <span class="c1"># (N, 3)
</span>        <span class="c1"># we softmax the logits to get predicted probabilities
</span>        <span class="n">prob</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>    <span class="c1"># (N, 3)
</span>        <span class="c1"># we compute the cross entropy loss
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">label</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">prob</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span>
</code></pre></div></div>

<p>To see the performance gain brought by reward prediction, let’s compare a vanilla A3C agent and one augmented with reward prediction on a series of maze tasks in DM Lab. The augmented agent significantly outperforms the vanilla one by 33%. In my experience, the task of reward prediction in DM Lab tasks is fairly easy to learn for agents because the semantic information is quite straightforward. For instance, an agent can predict positive when it sees a green and round object (an Apple in DM Lab, which is worth of +1 reward) or an orange and stacked object (the Goal in DM Lab, which is worth of +10 reward) and predict negative when it sees a yellow and round object (a Lemon, which is of -1 reward). Thus agents learn the representation of features in a manner similar to object detection. However, in environments where semantic information relating to rewards is much more complicated and abstract, the performance on this auxiliary task degrades and becomes harder to converge. A simple way to verify this phenomenon is to replace the RGB observation, from which the reward label is predicted, by depth observation in which rewarding objects should be learned through their shapes instead of colors.  Therefore, I reckon that although this auxiliary task is generally applicable, the difficulty of learning it and the performance gain it can bring may depend on environment complexity and the modality of sensorimotor inputs.</p>

<p class="center"><img src="/assets/img/posts/2021-03-14-AuxTasks/rp_compare.png" alt="image" style="zoom:30%;" /></p>

<h2 id="forward--inverse-dynamics">Forward &amp; Inverse Dynamics</h2>

<p>Generating intrinsic reward signal is a way in RL to help agents explore more novel states and to encourage them to perform actions which can reduce uncertainties in their belief states. In 2017, <a href="https://arxiv.org/abs/1705.05363" target="_blank">Pathak <em>et al</em>.</a> proposed to use curiosity as an intrinsic signal which they defined as the error of the agent’s prediction of the consequences of its own actions. To avoid falling into an <em>artificial curiosity trap</em>, an agent should embed observations into a feature space where only takes account of things under its control or things can affect it (imagining that there is a television showing random Gaussian noise, an agent rewarded by its prediction discrepancy but cannot distinguish task-irrelevant variations will do nothing but watch that TV forever). A forward dynamics model and an inverse dynamics model are proposed to learn that feature space. Their corresponding proxy objectives and the main RL object are jointly optimized during training. A forward dynamics model \(f(\cdot;\theta_F)\) is a predicator of the feature representation of the next observation \(\phi(o_{t+1})\) given the feature representation of the current observation \(\phi(o_t)\) and the action taken \(a_t\). To build such a model, an agent should have enough knowledge about the underlying environments in a learnable embedded space. But how can it learn such a space that only embeds task-relevant information and features that can be controlled by itself? The inverse dynamics model provides extra self-supervised signals to do that. An inverse dynamics model \(g(\cdot ; \theta_I)\) takes representations of successive observations \(\phi(o_t)\) and \(\phi(o_{t+1})\) and tries to tell which action \(\hat{a_t}\) causes that transition. By concurrently training these two models, the encoder \(\phi(\cdot):\mathcal{O} \rightarrow \mathcal{F}\) will be good enough at neglecting task-irrelevant and uncontrollable variations and encoding from observation space \(\mathcal{O}\) to feature space \(\mathcal{F}\) in which the agent can predict consequences of its actions.</p>

<p class="center"><img src="/assets/img/posts/2021-03-14-AuxTasks/noise_tv.jpeg" alt="image" style="zoom:30%;" /></p>

<p>Now let’s look at the forward and inverse dynamics models and auxiliary tasks alongside with them in more details. The diagram below, which is from the original paper of <a href="https://arxiv.org/abs/1705.05363" target="_blank">Pathak <em>et al</em>.</a>, shows how they work. The observation at time step <em>t</em> is RGB or RGBD image. At each time step, the forward dynamics model predicts the feature representation of the next observation. The discrepancy between the prediction and the true feature representation of the successive observation is used as an intrinsic bonus awarded to the agent, i.e., \(r_t^i=\eta \left\Vert \hat{\phi}(s_{t+1}) - \phi(s_{t+1}) \right\Vert_2^2\), where \(\hat{\phi}(s_{t+1}) = f\left(\phi(s_t), a_t;\theta_F \right)\). Parameters of the forward dynamics model is optimized by minimizing the MSE loss:</p>

\[\theta_F = \mathop{\mathrm{arg\,min}}_{\theta_F} \frac{1}{2} \left\Vert f\left(\phi(s_t), a_t;\theta_F \right) - \phi(s_{t+1}) \right\Vert^2_2.\]

<p>The inverse dynamics model is fed with feature representations of successive observations and tries to predict the action causing that transition, i.e., \(\hat{a}_t = g\left(\phi\left(s_t \right), \phi \left(s_{t+1}\right); \theta_I \right)\). Parameters \(\theta_I\) is optimized in a maximum-likelihood manner under <a href="https://en.wikipedia.org/wiki/Multinomial_distribution" target="_blank">multinomial distributions</a>:</p>

\[\theta_I = \mathop{\mathrm{arg\,min}}_{\theta_I} \sum^{\left\vert\mathcal{A} \right\vert}_i -a_t^{(i)} \cdot \log P_t^{(i)},\]

<p>where \(a_t^{(i)}\) and \(P_t^{(i)}\) is the indicator of the <em>i</em>-th action entry and predicted probability of that entry at time step <em>t</em>, respectively. Hence, the auxiliary tasks introduced can be trained at the same time with the main RL task by including the above two objectives.</p>

<p class="center"><img src="/assets/img/posts/2021-03-14-AuxTasks/icm.png" alt="image" style="zoom:30%;" /></p>

<p>In the code demo below, we assume that the encoder $\phi$ is a typical CNN which outputs flattened features and that forward and inverse models are MLPs. The forward MLP firstly concatenates the embedded observation at time step <em>t</em> with one-hot encoded action sampled from policy and then outputs a vector with the same size as the embedded observation. The inverse dynamics model takes input from concatenated vectors of embedded successive observations and outputs logits used to produce a categorical distribution of actions.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>


<span class="k">class</span> <span class="nc">ICM</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="s">"""
    Intrinsic Curiosity Module in Curiosity-driven Exploration by Self-supervised Prediction, Pathak et al., ICML 2017
    https://arxiv.org/abs/1705.05363
    """</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="s">"""
        Args:
            model (nn.Module): neural network model which has attributes `encoder`, `forward_dyn`, and `inverse_dyn`.
        """</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_mse_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MSELoss</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_nll_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">NLLLoss</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_log_softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward_dynamics_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="s">"""
        Args:
            obs (torch.Tensor): image observations with shape (N, L + 1, C, H, W)
            action (torch.Tensor): one-hot encoded actions with shape (N, L, A), where A is the number of actions
        """</span>
        <span class="c1"># first we combine N &amp; L axes
</span>        <span class="n">N</span><span class="p">,</span> <span class="n">L</span> <span class="o">=</span> <span class="n">action</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">action</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">obs</span> <span class="o">=</span> <span class="n">obs</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">L</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="o">*</span><span class="n">obs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]))</span>    <span class="c1"># (N, L + 1, C, H, W) -&gt; (N * (L + 1), C, H, W)
</span>
        <span class="n">obs_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_model</span><span class="p">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>    <span class="c1"># (N * (L + 1), -1)
</span>        <span class="c1"># recover N &amp; L axes
</span>        <span class="n">obs_emb</span> <span class="o">=</span> <span class="n">obs_emb</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">L</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>    <span class="c1"># (N * (L + 1), -1) -&gt; (N, L + 1, emb_size)
</span>
        <span class="c1"># concatenate obs_emb with one-hot encoded action as input to forward dynamics model
</span>        <span class="n">emb_obs_with_action</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">obs_emb</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">action</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>    <span class="c1"># (N, L, 1 + emb_size)
</span>        <span class="c1"># predict embedding of the next obs
</span>        <span class="n">pred_emb_obs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_model</span><span class="p">.</span><span class="n">forward_dyn</span><span class="p">(</span><span class="n">emb_obs_with_action</span><span class="p">)</span>    <span class="c1"># (N, L, emb_size)
</span>        <span class="c1"># compute the forward dynamics loss
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_mse_loss</span><span class="p">(</span><span class="n">pred_emb_obs</span><span class="p">,</span> <span class="n">obs_emb</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:])</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">inverse_dynamics_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="s">"""
        Args:
            obs (torch.Tensor): image observations with shape (N, L + 1, C, H, W)
            action (torch.Tensor): one-hot encoded actions with shape (N, L, A), where A is the number of actions
        """</span>
        <span class="c1"># first we combine N &amp; L axes
</span>        <span class="n">N</span><span class="p">,</span> <span class="n">L</span> <span class="o">=</span> <span class="n">action</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">action</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">obs</span> <span class="o">=</span> <span class="n">obs</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">L</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="o">*</span><span class="n">obs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]))</span>    <span class="c1"># (N, L + 1, C, H, W) -&gt; (N * (L + 1), C, H, W)
</span>
        <span class="n">obs_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_model</span><span class="p">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>    <span class="c1"># (N * (L + 1), -1)
</span>        <span class="c1"># recover N &amp; L axes
</span>        <span class="n">obs_emb</span> <span class="o">=</span> <span class="n">obs_emb</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">L</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>    <span class="c1"># (N * (L + 1), -1) -&gt; (N, L + 1, emb_size)
</span>
        <span class="c1"># concatenate successive obs_emb
</span>        <span class="n">suc_obs_emb</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">obs_emb</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">obs_emb</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>    <span class="c1"># (N, L, 2 * emb_size)
</span>        <span class="c1"># compute logits
</span>        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_model</span><span class="p">.</span><span class="n">inverse_dyn</span><span class="p">(</span><span class="n">suc_obs_emb</span><span class="p">)</span>    <span class="c1"># (N, L, A)
</span>        <span class="c1"># compute cross entropy loss
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_nll_loss</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">_log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">N</span> <span class="o">*</span> <span class="n">L</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))),</span>     <span class="c1"># log prob
</span>            <span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">action</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">N</span> <span class="o">*</span> <span class="n">L</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>    <span class="c1"># target
</span>        <span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>
</code></pre></div></div>

<p>Now let’s see the comparison results between an A3C agent augmented with these two auxiliary tasks, one variant only augmented with forward dynamics model, and a vanilla A3C agent. The one only with forward dynamics model predicts observations in pixel space and uses prediction errors as intrinsic reward. It shows that the full agent (ICM + A3C) achieves the best performance and enjoys the fastest learning speed in all settings with different sparsity of extrinsic reward. Furthermore, in the hardest setting, i.e., very sparse reward setting, only the full agent can solve the task. The one predicting states in pixel space, i.e., ICM (pixels) + A3C, fails potentially due to the increased complexity of visual textures or even due to the unclarity of the effectiveness of predicting pixels as the objective. This is similar to the phenomenon happened in Pixel Control we just discussed where an agent augmented with input reconstruction performs badly.</p>

<p class="center"><img src="/assets/img/posts/2021-03-14-AuxTasks/curiosity_results.png" alt="image" style="zoom:30%;" /></p>

<h2 id="cpca">CPC|A</h2>

<p><a href="https://arxiv.org/abs/1807.03748" target="_blank">Contrastive Predictive Coding</a> (CPC) is an unsupervised learning approach used to extract representations from high-dimensional data such as images. Instead of modeling a generative model \(p_k\left(x_{t+k} \vert c_t\right)\) between future observations \(x_{t+k}\) and context \(c_t\), it models a density ratio function \(f_k \left(x_{t+k}, c_t\right) \propto \frac{p_k\left(x_{t+k} \vert c_t\right)}{p_k\left(x_{t+k} \right)}\) by maximizing the <a href="https://en.wikipedia.org/wiki/Mutual_information" target="_blank">mutual information</a> between \(x_{t+k}\) and \(c_t\), i.e.,</p>

\[\begin{align}
I\left(x_{t+k};c_t \right) &amp;= \sum_{x_{t+k}, c_t} p_k \left(x_{t+k},c_t \right) \log \frac{p_k \left(x_{t+k} , c_t \right)}{p_k \left(x_{t+k} \right) p_k \left(c_t\right)} \\
&amp;=\sum_{x_{t+k}, c_t} p_k \left(x_{t+k},c_t \right) \log \frac{p_k \left(x_{t+k} \vert c_t \right)}{p_k \left(x_{t+k} \right)}.
\end{align}\]

<p>This is done by minimizing a loss function based on <a href="http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf" target="_blank">NCE</a>, named as <em>InfoNCE</em>:</p>

\[\mathcal{L}_N = - \mathbb{E}_X \left[\log \frac{f_k \left(x_{t+k}, c_t\right)}{\sum_{x_j \in X}f_k \left(x_j, c_t \right)} \right].\]

<p>Intuitively, minimizing this loss leads to the probability begin maximized of classifying a positive sample from the conditional distribution \(p_k\left(x_{t+k} \vert c_t\right)\).</p>

<p>CPC|A (CPC conditioned on actions) is proposed by <a herf="https://arxiv.org/abs/1811.06407" target="_blank">Guo <em>et al.</em></a> in 2018. When implementing CPC, practicers can use an autoregressive model such as LSTM and GRU as the density ratio function \(f_k \left(x_{t+k}, c_t\right)\). Similarly, the density ratio function in CPC|A can also be implemented using a RNN, except that future actions are fed to it. Concretely, at time step \(t\), the CPC|A GRU is initialized with the hidden state \(b_t\) of a belief module (e.g., an LSTM) which summarizes histories of observations and actions. Then, it is unrolled for \(T\) steps and fed by a series of future actions \(\{a_{t+i}\}_{i=0}^{T-1}\). Outputs at each future step are concatenated with either an embedding of positive observations (e.g., the RGB image observed at that time step) or an embedding of negative observations (e.g., RGB images randomly drawn from a buffer). An MLP classifier is asked to predict 1 for positive embeddings and 0 for negative embeddings. The diagram below, which is taken from <a herf="https://arxiv.org/abs/1811.06407" target="_blank">Guo <em>et al.</em></a>,  gives an illustration.</p>

<p class="center"><img src="/assets/img/posts/2021-03-14-AuxTasks/cpca.png" alt="image" style="zoom:50%;" /></p>

<p>I implements CPC|A in the following code block with reference to this <a href="https://github.com/joel99/habitat-pointnav-aux" target="_blank">repo</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>


<span class="k">class</span> <span class="nc">ActionConditionedCPC</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Implementation of CPC|A.
    https://arxiv.org/abs/1811.06407
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">gru_hidden_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">gru_input_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">mlp_hidden_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">feature_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">k</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="s">"""
        Args:
            gru_hidden_size (int): hidden size of CPC|A GRU.
            gru_input_size (int): input size of CPC|A GRU.
            mlp_hidden_size (int): hidden size of the MLP classifier.
            feature_size (int): output size of the feature encoder (e.g., CNN).
            k (int): predict k steps in the future.
        """</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ActionConditionedCPC</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="c1"># create CPC|A GRU
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">_gru_cpca</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">GRU</span><span class="p">(</span>
            <span class="n">input_size</span><span class="o">=</span><span class="n">gru_input_size</span><span class="p">,</span>
            <span class="n">hidden_size</span><span class="o">=</span><span class="n">gru_hidden_size</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># create an MLP classifier
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">_classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">feature_size</span> <span class="o">+</span> <span class="n">gru_hidden_size</span><span class="p">,</span> <span class="n">mlp_hidden_size</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">mlp_hidden_size</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">_k</span> <span class="o">=</span> <span class="n">k</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">_cross_entropy_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">obs_emb</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">act_emb</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">init_hidden_state</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="s">"""
        Args:
            obs_emb (torch.Tensor): embeddings of future observations with shape (N, L + 1, feature_size) where L &gt;= k.
            act_emb (torch.Tensor): embeddings of future actions with shape (N, L, gru_input_size) where L &gt;= k.
            init_hidden_state (torch.Tensor): hidden state used to initialize CPC|A GRU with shape (N, gru_hideen_size)
        """</span>
        <span class="n">N</span><span class="p">,</span> <span class="n">L</span> <span class="o">=</span> <span class="n">act_emb</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">act_emb</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">assert</span> <span class="n">L</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_k</span>

        <span class="c1"># positive embeddings are those of real obs at future time steps
</span>        <span class="c1"># obs_emb[:, 0] is the embedding of obs at time step t
</span>        <span class="c1"># so we start from 1 for embedding of future obs
</span>        <span class="n">pos_emb</span> <span class="o">=</span> <span class="n">obs_emb</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>

        <span class="c1"># negative embeddings are embeddings of obs not at those time steps
</span>        <span class="c1"># they can be drawn from a buffer
</span>        <span class="c1"># but here we use permuted positive embeddings
</span>        <span class="n">perm_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randperm</span><span class="p">(</span><span class="n">N</span> <span class="o">*</span> <span class="n">L</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">obs_emb</span><span class="p">.</span><span class="n">device</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">neg_emb</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span>
            <span class="n">obs_emb</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:].</span><span class="n">reshape</span><span class="p">((</span><span class="n">N</span> <span class="o">*</span> <span class="n">L</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span>
            <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">index</span><span class="o">=</span><span class="n">perm_idx</span><span class="p">.</span><span class="n">repeat</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">obs_emb</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
        <span class="p">)</span>
        <span class="n">neg_emb</span> <span class="o">=</span> <span class="n">neg_emb</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="c1"># now we ask the GRU to unroll k-steps into the future
</span>        <span class="n">act_emb</span> <span class="o">=</span> <span class="n">act_emb</span><span class="p">[:,</span> <span class="p">:</span><span class="bp">self</span><span class="p">.</span><span class="n">_k</span><span class="p">]</span>    <span class="c1"># (N, K, -1)
</span>        <span class="n">gru_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_gru_cpca</span><span class="p">(</span>
            <span class="n">act_emb</span><span class="p">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
            <span class="n">init_hidden_state</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="n">gru_out</span> <span class="o">=</span> <span class="n">gru_out</span><span class="p">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>    <span class="c1"># (K, N, -1) -&gt; (N, K, -1)
</span>
        <span class="c1"># assemble inputs for the MLP classifier
</span>        <span class="n">pos_inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">pos_emb</span><span class="p">,</span> <span class="n">gru_out</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>    <span class="c1"># (N, K, feature_size + gru_hidden_size)
</span>        <span class="n">neg_inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">neg_emb</span><span class="p">,</span> <span class="n">gru_out</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>    <span class="c1"># (N, K, feature_size + gru_hidden_size)
</span>        <span class="c1"># combine N &amp; K axes
</span>        <span class="n">pos_inputs</span> <span class="o">=</span> <span class="n">pos_inputs</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">pos_inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">neg_inputs</span> <span class="o">=</span> <span class="n">neg_inputs</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">neg_inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

        <span class="c1"># get logits from the MLP classifier
</span>        <span class="n">pos_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_classifier</span><span class="p">(</span><span class="n">pos_inputs</span><span class="p">)</span>    <span class="c1"># (N * K, 2)
</span>        <span class="n">neg_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_classifier</span><span class="p">(</span><span class="n">neg_inputs</span><span class="p">)</span>    <span class="c1"># (N * K, 2)
</span>
        <span class="c1"># get loss
</span>        <span class="n">pos_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_cross_entropy_loss</span><span class="p">(</span>
            <span class="n">pos_logits</span><span class="p">,</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">pos_logits</span><span class="p">).</span><span class="nb">long</span><span class="p">()[:,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">neg_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_cross_entropy_loss</span><span class="p">(</span>
            <span class="n">neg_logits</span><span class="p">,</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">neg_logits</span><span class="p">).</span><span class="nb">long</span><span class="p">()[:,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">pos_loss</span> <span class="o">+</span> <span class="n">neg_loss</span>
</code></pre></div></div>

<p><a href="https://arxiv.org/abs/2007.04561" target="_blank">Ye <em>et al.</em></a> claims an improvement of 0.12 SPL (<a href="https://arxiv.org/abs/1807.06757" target="_blank">Success weighted by Path Length</a>) for task <code class="language-plaintext highlighter-rouge">PointNavigation</code> in <a href="https://arxiv.org/abs/1904.01201" target="_blank">Habitat</a> by augmenting CPC|A-16 (predicting 16 future steps) over a <a href="https://arxiv.org/abs/1911.00357" target="_blank">DD-PPO</a> baseline. The augmented agent enjoys higher data efficiency as well. The figure below is taken from <a href="https://arxiv.org/abs/2007.04561" target="_blank">Ye <em>et al.</em></a> to demonstrate the improvement brought by CPC|A.</p>

<p class="center"><img src="/assets/img/posts/2021-03-14-AuxTasks/cpca_results.png" alt="image" style="zoom:30%;" /></p>

<h2 id="auxiliary-predictive-modeling-tasks">Auxiliary Predictive Modeling Tasks</h2>

<p>Human is able to encode, consolidate, and store streams of sensorimotor inputs (e.g., what you see, what you hear, what you touch, what you taste, etc.) and retrieve these stored information for predictions of upcoming events. For example, once you tried a dish with impressive taste. Next time you see it, you probably will recall that taste. <a href="http://docshare04.docshare.tips/files/29847/298473401.pdf" target="_blank">Neuroscientists</a> found that <a href="https://en.wikipedia.org/wiki/Hippocampus" target="_blank">hippocampus</a> in brains participates in the encoding, consolidation, and retrieval of memories. Similarly, in the world of RL agents, memory is necessary for agents to tackle with partial observability existed in raw sensory data. With the idea of predictive modeling, <a href="https://arxiv.org/abs/1803.10760" target="_blank">Wayne <em>et al.</em></a> proposed a <em>memory-based predictor</em> (MBP) which compresses and stores observations and retrieves a series of history observations to make predictions. Similar to model the marginal distribution \(\log p \left( x\right)\), the MBP is trained toward predicting consistently with the probabilities of observed sensorimotor streams from the environment. Sadly, this objective is intractable to be optimized. Nevertheless, the surrogate objective <a href="https://yjiang05.github.io/ELBO/" target="_blank">ELBO</a> can be used for the optimization of MBP. As we know that ELBO consists of two term, one is the reconstruction loss and the other serves as a regularization against the complexity of variational posterior. When exposing steams with multiple modalities, e.g., images, text instructions, velocities, the reconstruction loss can be a combination of all of them. These reconstruction tasks can be considered as auxiliary reconstruction tasks, which are done by additional heads trained under individual reconstruction loss.</p>

<p>Considering training RL agents in <a href="https://github.com/deepmind/lab" target="_blank">DM Lab</a> as in <a href="https://arxiv.org/abs/1803.10760" target="_blank">Wayne <em>et al.</em></a>, they get text instructions, observe images, and know their last action and corresponding reward. Some extra information about their dynamics such as velocity is also available. The MBP is trained via optimizing ELBO such that distributions of these observations can be modeled.  Specifically, we build an image decoder, a return prediction decoder, a text decoder, a reward decoder, a velocity decoder and an action decoder. Input to these decoders is one realization from the posterior distribution \(z_t \sim \mathcal{N}\left(\mu_t^q, \Sigma_t^q\right)\). Each decoder tries to reconstruct the observation in corresponding modality. Assuming independency between these loss terms, the negative conditional log-likelihood can be expressed as</p>

\[\begin{align}
-\log p \left(o_t, R_t \vert z_t \right) &amp;\equiv \alpha_{image} \mathcal{L}_{image} + \alpha_{return}\mathcal{L}_{return} + \alpha_{reward}\mathcal{L}_{reward}\\ 
&amp;+ \alpha_{action}\mathcal{L}_{action} + \alpha_{velocity}\mathcal{L}_{velocity} + \alpha_{text} \mathcal{L}_{text}.
\end{align}\]

<p>Except the reconstruction loss terms, the variational posterior is also regularized to the prior distribution through KL divergence \(D_{KL} \left(\mathcal{N}\left(\mu_t^q, \Sigma_t^q\right)\Vert \mathcal{N} \left(\mu_t^p, \Sigma_t^p\right) \right)\), resulting in a lower bound to the MBP loss:</p>

\[\mathcal{L}_t \geq \mathbb{E}_{z_{\tau} \sim \mathbb{Q}_t} \left[\log p \left( o_{\tau}, R_\tau \vert z_\tau	\right) - D_{KL} \left(\mathbb{Q}_t, \Vert \mathbb{P}_t \right)\right].\]

<p>Next, let’s see one example of comparison between an agent augmented with predictive modeling (<a href="https://arxiv.org/abs/1803.10760" target="_blank"><em>MERLIN</em></a>) and two baseline agents. This result is from <a href="https://arxiv.org/abs/1803.10760" target="_blank">Wayne <em>et al.</em></a>, where the solid orange line represents MERLIN, the blue represents a RL-LSTM agent, the pink represents a RL-MEM agent, and the dotted orange shows the negative ELBO.</p>

<p class="center"><img src="/assets/img/posts/2021-03-14-AuxTasks/merlin.png" alt="image" style="zoom:40%;" /></p>

<p>This task is <a href="https://github.com/deepmind/lab/tree/master/game_scripts/levels/contributed/dmlab30#watermaze" target="_blank">Watermaze</a> which tests the memory ability of agents. As we can see as the training goes on, the episode score increases while the negative VLB (variational lower bound, same as ELBO) decreases. The positive correlation between the performance and ELBO suggests that as the MBP can model and predict sensorimotor steams more accurately, the agent can do better in the memory task. The more goal-directed behavior in turn facilitates the learning of the MDP as the agent is more likely to be exposed to upcoming streams whose distributions are becoming more and more predictable (i.e., the ongoing observations will become more correlated to positive rewards). By contrast, two baseline agents fail to solve this task.</p>

<p>More details about MERLIN will be discussed in a future post.</p>

<h2 id="auxiliary-policies">Auxiliary Policies</h2>

<p>We can even build auxiliary policies to augment the main policy of the agent by leveraging <a href="https://arxiv.org/abs/1905.01240" target="_blank">information asymmetry</a>. <a href="https://arxiv.org/abs/2006.15223" target="_blank">Stooke <em>et al.</em></a> proposed a <em>Perception-Prediction-Reaction</em> (PPR) agent by employing a temporal hierarchy and distance between main and auxiliary policies as a regularization.  Concretely, starting from the <a href="https://arxiv.org/abs/1807.01281" target="_blank">minimal temporal hierarchy</a> which is essentially two RNNs operating at different time scale to divide and simplify the responsibilities of long-term and short-term memory, PPR utilizes two more fast-ticking RNNs (i.e., operating at the same time scale as the RNN for main policy) with different memory accessibility to create an asymmetry. Specifically, one fast-ticking core is a sensory loop, namely <em>perception</em>, which only takes sensorimotor observations as inputs and outputs to the slow-ticking RNN. Note that it has no access to the long-term memory. The other fast-ticking core is a motor loop, namely <em>prediction</em>, which can access the long-term memory but not the observation steams. Its outputs will not go to the slow-ticking RNN neither. Finally, the fast-ticking core, <em>Reaction</em>, takes observations and long-term memory as inputs and outputs the main policy for the agent. Its outputs will also not go to the slow-ticking core. These architectural design introduces structural priors on RNN core and creates information asymmetry to regularize both main and auxiliary policies and help to shape the hidden representations. The figure below which is directly from <a href="https://arxiv.org/abs/2006.15223" target="_blank">Stooke <em>et al.</em></a> illustrates this architecture.</p>

<p class="center"><img src="/assets/img/posts/2021-03-14-AuxTasks/ppr.png" alt="image" style="zoom:40%;" /></p>

<p>Let’s discuss how the auxiliary loss works. In <a href="https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process" target="_blank">POMDPs</a>, agents can improve their understanding about the current state by incorporating the history of observations and maintaining a hidden state \(h\). It is done in a autoregressive manner with RNN \(h_t=f\left(x_t, h_{t-1}\right)\). Hence, the policy changes from \(\pi\left(a_t \vert s_t\right)\) to \(\pi \left(a_t \vert h_t\right)\). Leveraging temporal hierarchy provided by slow-ticking and fast-ticking cores, i.e., \(h_t^S\) and \(h_t^F\), the policy can handle tasks which require memory ability spanning across longer time axis \(\pi\left(a_t \vert h_t^S, h_t^F\right)\). Following the idea of RL as <a href="https://papers.nips.cc/paper/2013/hash/38af86134b65d0f10fe33d30dd76442e-Abstract.html" target="_blank">probabilistic inference</a> and assuming polices as multivariate Gaussian distributions, the main policy from <em>Reaction</em> branch and auxiliary polices from <em>Perception</em> and <em>Prediction</em> branches can be obtained:</p>

\[\begin{align}
&amp;\pi_t = \mathcal{N}\left(\mu_t, \Sigma_t\right) \\
&amp;\pi_t'= \mathcal{N} \left(\mu_t', \Sigma_t'\right) \\
&amp;\pi_t''= \mathcal{N} \left(\mu_t'', \Sigma_t''\right) \\
\end{align},\]

<p>where</p>

\[\begin{align}
&amp;\mu_t, \Sigma_t = g\left(h_t^{Reaction}\right) \\
&amp;\mu_t', \Sigma_t' = g'\left(h_t^{Perception}\right) \\
&amp;\mu_t'', \Sigma_t'' = g''\left(h_t^{Prediction}\right)
\end{align}.\]

<p>To encourage each policy to agree as much as well with other policies despite the different information accessibility, the auxiliary loss contains three KL divergence terms between each two of them:</p>

\[\begin{align}
\mathcal{L}_{aux} = \sum_t &amp;D_{KL} \left(\mathcal{N}\left(\mu_t, \Sigma_t \right) \Vert \mathcal{N}\left(\mu_t', \Sigma_t'\right)\right) + \\ 
&amp;D_{KL} \left(\mathcal{N}\left(\mu_t, \Sigma_t \right) \Vert \mathcal{N}\left(\mu_t'', \Sigma_t''\right)\right) + \\
&amp;D_{KL} \left(\mathcal{N}\left(\mu_t', \Sigma_t' \right) \Vert \mathcal{N}\left(\mu_t'', \Sigma_t''\right)\right).
\end{align}\]

<p>Intuitively, this auxiliary loss puts two priors on the main policy. Despite the information asymmetry, the main policy should be inferable from an auxiliary policy which only incorporates recent sensorimotor steams, i.e., policy from <em>perception</em>. It also should be expressible from a policy which only accesses long-term memory and cannot access any instant observations, i.e., policy from <em>prediction</em>.</p>

<p class="center"><img src="/assets/img/posts/2021-03-14-AuxTasks/ppr_results.png" alt="image" style="zoom:50%;" /></p>

<p>The above experiment results are from <a href="https://arxiv.org/abs/2006.15223" target="_blank">Stooke <em>et al.</em></a>, where the PPR agent is compared with a baseline <a href="https://arxiv.org/abs/1802.01561" target="_blank">IMPALA agent</a>. We can see that in memory-based levels such as <a href="https://github.com/deepmind/lab/tree/master/game_scripts/levels/contributed/dmlab30#select-non-matching-object" target="_blank">Select Non-maching Object</a>, <a href="https://github.com/deepmind/lab/tree/master/game_scripts/levels/contributed/dmlab30#watermaze" target="_blank">Watermaze</a>, and <a href="https://github.com/deepmind/lab/blob/master/game_scripts/levels/nav_maze_random_goal_03.lua" target="_blank">Navmaze</a>, the PPR agent significantly outperforms the baseline agent which only utilizes one recurrent module to deal with long-term memory, suggesting the extra auxiliary policies and loss improve the memory ability of the agent. In reactive levels such as <a href="https://github.com/deepmind/lab/blob/master/game_scripts/levels/lt_hallway_slope.lua" target="_blank">Laser tag</a>, the PPR agent does not show any degradation given that the policy becomes less dynamics because it is trained to be easier to be predicted.</p>

<h2 id="summary">Summary</h2>

<p>In this post, we have reviewed plenty of auxiliary tasks used commonly in RL. We have familiarized with their concepts, known how they work, investigated improvements they bring, and examined how they are implemented. Despite their name “<em>auxiliary</em>” which means “<em>supplementary</em> “ — which means the optimization is still dominated by the RL objective — it also means “offering or providing help” — considering the fact that they do improve the sample efficiency and speed up the learning in many tasks. I sincerely hope that this post can help you understand better if you are not very familiar with them or assist your work through the demo code snippets. Please feel free to raise any questions or point out any errors or mistakes. Lastly, thanks for your interest and reading and please stay tuned for further posts.</p>

  </div><a class="u-url" href="/2021/03/14/AuxTasks.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Yunfan Jiang&#39;s Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Yunfan Jiang&#39;s Blog</li><li><a class="u-email" href="mailto:yunfanjiang05@gmail.com">yunfanjiang05@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/yjiang05" target="_blank"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">yjiang05</span></a></li><li><a href="https://www.linkedin.com/in/yunfanjiang" target="_blank"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">yunfanjiang</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Hi there, thanks for visiting. This is my Blog where I summarize my readings, express and share my ideas, record my progress, and so on. Wish you a nice journal in my Blog!</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
