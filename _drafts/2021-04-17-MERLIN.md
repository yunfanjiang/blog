---
layout: post
title: Hello MERLIN
---

<a href='https://arxiv.org/abs/1803.10760' target='_blank'><i>Memory, RL, and Inference Network</i></a> Explained

In this post, I'll talk about the _MERLIN_ agent introduced in the paper <a href='https://arxiv.org/abs/1803.10760' taget='_blank'>Unsupervised Predictive Memory in a Goal-Directed Agent</a> by Wayne _et al._. It combines external memories, RL, and probabilistic inference and achieves several amazing results in behavioral tasks in psychology and neurobiology. I wish to share you with my understanding and thinking about _MERLIN_ and hope that it could help your study. Don't be scared by the number of pages of that paper and let's start now.

## Quick Facts

* _MERLIN_ is trained in an on-policy manner. It uses <a href='https://arxiv.org/abs/1506.02438' target='_blank'>GAE</a> to estimate the advantage as one part of the policy gradient.
* _MERLIN_ can be considered as an inclusion of a latent variable model. It approximates the posterior distribution of the state variable $z_t$. The policy and value networks are conditioned on state variables sampled using <a href='https://arxiv.org/abs/1312.6114' target='_blank'>reparameterization trick</a>.
* _MERLIN_ employs a stochastic policy.

## Table of Contents

* <a href='#game_memory'>The Game _Memory_ — A Challange to RL Agents</a>
* <a href='#combine'>Combining Inference, Memory, and RL</a>
	* <a href='#predictive_modelling'>Sensory Stream Compression & Unsupervised Predictive Modelling</a>

<h2 id='game_memory'>The Game <i>Memory</i> — A Challange to RL Agents</h2>

You may have played the game <a href='https://en.wikipedia.org/wiki/Concentration_(card_game)' target='_blank'>_Memory_</a> before (if not, you can play it <a href='https://matchthememory.com/' target='_blank'>here</a>). In this game, all cards are placed face down. Once you select a card, that card flips over and then flips back. Two successive selections with the same pattern will result in one point and those two cards will be removed. As its name suggests, playing this game requires "memory". The typical way to provide RL agents with memory is to augment RNNs: observations at time step $$t$$ are embedded into features $$e_t$$ which are subsequently fed into the RNN. The agent updates its internal state by $$h_t = f\left(e_t, h_{t-1}\right)$$. The policy and value networks are conditioned on this hidden state, i.e., $$\pi_t\left(\cdot \vert h_t\right)$$ and $$V_t \left(\cdot \vert h_t\right)$$. We can further improve the memory capability by augmenting an external memory (we will discuss how it works later). Sadly, neither the vanilla RL-LSTM nor the more complicated RL-MEM could solve the game.

Formally, we call tasks resemble the game _Memory_ where sufficient information is concealed to agents as  <a href='https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process' taget='_blank'>_POMDPs_</a>. Memories are required to solve tasks featuring partial observability. However, in the vanilla RL-LSTM agent, the duration within which the algorithm can do credit assignment is the window size used in the <a href='https://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf' target='_blank'>truncated BPTT</a>. Therefore, the RL-LSTM agent may fail to assign credits with temporal dependencies longer than the window size. As for the RL-MEM agent, despite the external memory, it is still trained in a trial-and-error fashion based on rewards received. This is indirect and insufficient. Instead, _the right information should be stored in the right format_. We will discuss how this is achieved in the following sections.

<h2 id='combine'>Combining Inference, Memory, and RL</h2>

_MERLIN_ combines conventional RL optimized with policy gradient with external memories and state estimation/inference. In this section, we will see how the process of predictive modeling can guide the formation of memory, which further helps the RL training by alleviating the hard problem of credit assignment.

<h3 id='predictive_modelling'>Sensory Stream Compression & Unsupervised Predictive Modelling</h3>

In supervised learning, people always compress sensory inputs to force the neural network to capture the data structure. For example, an autoencoder learning an identity function $$\hat{x} = f\left(x\right)$$ can learn compressed representations of inputs by having a "bottleneck" layer that has a much smaller dimension than inputs/outputs. The same idea can be found in _MERLIN_ as well. At each time step, the _MERLIN_ agent is exposed with an image showing the first-person view,  a velocity vector comprising the translational and rotational velocities, the previous action it takes, the previous reward it obtains, and a text instruction. Images are encoded through 6 blocks of <a href='https://arxiv.org/abs/1512.03385' target='_blank'>ResNet</a>, resulting in a 500-dimension vector. The velocity is encoded through a linear transformation with the output size of 10. One-hot previous action and previous reward are left untouched. Finally, text instructions are processed through an embedding layer of size 50 and a single-layer LSTM yielding outputs with the dimension of 100. The size of vocabulary in <a href='https://github.com/deepmind/lab' target='_blank'>DMLab</a> (i.e., the environment in which the _MERLIN_ is tested) is 1000 and the length of each instruction does not exceed 10. Outputs from all encoders are concatenated into single vectors $$e_t$$.



