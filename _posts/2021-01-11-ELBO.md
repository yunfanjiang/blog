---
layout: post
title: ELBO — What & Why
---

ELBO (evidence lower bound) is a key quantity in [Variational Bayesian Methods](https://en.wikipedia.org/wiki/Variational_Bayesian_methods). It transforms inference problems, which are always _intractable_, into optimization problems that can be solved with, for example, gradient-based methods.

<details>
    <summary><h2>Updates</h2></summary>
    <b>&#8226; April 13 2021</b>
    <p>Eq. (4) has been corrected in which the entropy term should be a conditional entropy of $Z$ given $X$, i.e., $H \left(Z \vert X \right)$, instead of $H \left(Z\right)$ in the previous version.</p>    
</details>


## Introduction

In this post, I'll introduce an important concept in  [Variational Bayesian (VB) Methods](https://en.wikipedia.org/wiki/Variational_Bayesian_methods) — ELBO (evidence lower bound, also known as variational lower bound) — and its derivations, alongside some digging into it. ELBO enables the rewriting of statistical inference problems as optimization problems, the so-called _inference-optimization duality_ (see [Eric Jang's blog](https://blog.evjang.com/2016/08/variational-bayes.html)). Combined with optimization methods such as gradient descent and modern approximation techniques, e.g., deep neural networks, inference on complex distributions can be achieved. Numerous applications can be found in [VAE](https://arxiv.org/abs/1312.6114v10), [DVRL](https://arxiv.org/abs/1806.02426), [MERLIN](https://arxiv.org/abs/1803.10760), to name a few.

## Table of Contents

* <a href='#motivation'>Motivation</a>
* <a href='#derivation_kl'>Derivation: From a View of KL Divergence</a>
* <a href='#derivation_logpx'>Derivation: From a View of Observations</a>
* <a href='#insight'>Insights</a>
* <a href='#summary'>Summaries</a>

<h2 id='motivation'>Motivation</h2>

Statistical inference problems focus on inferring the value of one random variable given the value of another random variable. Given the observation, $x$, we are interested in finding the _posterior_ distribution of the latent variable $z$ (we call it "latent" as it is not observed), $p(z \vert x)$. However, this is generally _intractable_ as when we express it using [Bayes' theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem), the computation of the marginal distribution of observation at the denominator requires the integration (or summation) over the whole latent space:

$$
\begin{align}
p(z|x)=\frac{p(z)p(x \vert z)}{p(x)}=\frac{p(z)p(x \vert z)}{\int_z p(x,z) dz}.
\tag{1}
\end{align}
$$

A natural question would be that "Can we use another distribution, let's say $q(z \vert x)$, as an approximation of the true _posterior_ distribution?" The answer is "Yes" and I shall explain in the next section.

<h2 id='derivation_kl'>Derivation: From a View of KL Divergence</h2>

Consider the problem that using a distribution $q_\theta(z \vert x)$ parameterized by $\theta$ to approximate the true _posterior_ distribution $p(z \vert x)$, the first issue we need to address is to select a suitable criterion to measure how close is $q_\theta(z \vert x)$ to $p(z \vert x)$. [Kullback–Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) is such a metric. Specifically, we use reverse KL divergence here (see [Eric Jang's blog](https://blog.evjang.com/2016/08/variational-bayes.html) for more insights into forward KL and reverse KL). 

The KL divergence between $q_\theta(z \vert x)$ and $p(z \vert x)$ is

$$
\begin{align}
D_{KL}\left(q_\theta(z \vert x) \Vert p(z \vert x)\right) &= \int_z q_\theta (z \vert x) \log \frac{q_\theta(z \vert x)}{p(z \vert x)} dz\\
&= -\int_z q_\theta(z \vert x) \log \frac{p(z \vert x)}{q_\theta(z \vert x)} dz\\
&= -\int_z q_\theta(z \vert x) \log \frac{p(z,x)}{q_\theta(z \vert x)p(x)} dz \\
&= - \left( \int_z q_\theta(z \vert x) \log \frac{p(z,x)}{q_\theta(z \vert x)} dz - \int_z q_\theta(z \vert x) \log p(x) dz\right) \\
&= - \int_z q_\theta(z \vert x) \log \frac{p(z,x)}{q_\theta(z \vert x)} dz + \log p(x) \\ 
&= \int_z q_\theta(z \vert x) \log \frac{q_\theta(z \vert x)}{p(z,x)} dz + \log p(x).
\tag{2}
\end{align}
$$

To minimize the KL divergence _w.r.t._ parameters $\theta$ we need to minimize the term $\int_z q_\theta(z \vert x) \log \frac{q_\theta(z \vert x)}{p(z,x)} dz$. It leads to maximize its negation $\mathcal{L} = -\int_z q_\theta(z \vert x) \log \frac{q_\theta(z \vert x)}{p(z,x)} dz$, which is known as the _evidence lower bound_ (ELBO). We will find some intuitive explanations about its name by plugging $\mathcal{L}$ into Eq. (2):

$$
\begin{align}
\log p(x) = \mathcal{L} + D_{KL}\left(q_\theta(z \vert x) \Vert p(z \vert x)\right).
\tag{3}
\end{align}
$$

By definition, the term "_evidence_" is the value of a likelihood function evaluated with fixed parameters. Note that the KL divergence at the right-hand side of Eq. (3) is non-negative, the term $\log p(x)$ is greater or equal to $\mathcal{L}$. And hence $\mathcal{L}$ sets a lower bound for evidence.

<h2 id='derivation_logpx'>Derivation: From a View of Observations</h2>

If we start the derivation from the marginal distribution of observations,

$$
\begin{align}
\log p(x) &= \log \int_z p(x, z) dz \\
&= \log \int_z p(x, z) \frac{q_\theta(z \vert x)}{q_\theta(z \vert x)} dz \\
&= \log \mathbb{E}_{z \sim q_\theta(z \vert x)} \left[ \frac{p(x, z)}{q_\theta(z \vert x)}\right] \\
&\geq \mathbb{E}_z \left[ \log \frac{p(x,z)}{q_\theta(z \vert x)}\right] \text{by Jensen's inequality} \\
&= \mathbb{E}_z \left[ \log p(x,z) \right] + \int_z q_\theta(z \vert x) \log \frac{1}{q_\theta(z \vert x)} dz \\
&= \mathbb{E}_z \left[ \log p(x,z) \right] + H(Z \vert X)
\tag{4},
\end{align}
$$

where $H(Z \vert X)$ is the conditional entropy. It is straightforward to show that the right hand of Eq. (4) is the same as $\mathcal{L}$, one expression of ELBO we derived in the previous section. It is also obvious that the term $\mathbb{E}_z \left[ \log p(x,z) \right] + H(Z \vert X)$ is a lower bound for the log-marginal probability of observations, which means that we just need to maximize ELBO when we wish to maximize the marginal probability of observations.

<h2 id='insight'>Insights</h2>

First of all, let's derive further from Eq. (4):

$$
\begin{align}
\mathcal{L} &= \mathbb{E}_{z \sim q_\theta(z \vert x)} \left[ \log p(x,z) \right] + H(Z \vert X) \\
&= \int_z q_\theta(z \vert x) \log \left(p(x|z)p(z)\right)dz + \int_z q_\theta(z \vert x) \log \frac{1}{q_\theta(z \vert x)} dz\\
&= \int_z q_\theta(z \vert x) \log p(x|z) dz + \int_z q_\theta(z \vert x) \log p(z) dz + \int_z q_\theta(z \vert x) \log \frac{1}{q_\theta(z \vert x)} dz\\
&= \mathbb{E}_z \left[ \log p(x|z)\right] - \int_z q_\theta(z \vert x) \log \left(\frac{q_\theta(z \vert x)}{p(z)}\right) dz \\
&= \mathbb{E}_z \left[ \log p(x|z)\right] - D_{KL}\left(q_\theta(z \vert x) \Vert p(z)\right).
\tag{5}
\end{align}
$$

Eq. (5) shows that the ELBO has two terms, one expected log-likelihood term and one KL divergence term. It suggests that the ELBO is a trade-off between the reconstruction accuracy against the complexity of the *posterior* approximation. The KL divergence term can be interpreted as a measure of the additional information required to express the *posterior* relative to the *prior*. As it approaches zero, the *posterior* is fully obtainable from the *prior*.

Furthermore, let's think about the reason behind the reverse KL divergence we used to derive Eq. (2):

$$
\begin{align}
D_{KL}\left(q_\theta(z \vert x) \Vert p(z \vert x) \right) = \int_z q_\theta(z \vert x) \log \frac{q_\theta(z \vert x)}{p(z \vert x)} dz.
\tag{6}
\end{align}
$$

It turns out that the variational distribution $q_\theta(z \vert x)$ is prevented from spanning the whole space. Consider the case where the denominator in Eq. (6) is zero, the value of $q_\theta(z \vert x)$ has to be zero as well otherwise the KL divergence goes to infinity. In summary, the reverse KL divergence has the effect of zero-forcing as minimizing it leads to $q_\theta (z \vert x)$ being squeezed under $p (z \vert x)$ (see [Eric Jang's blog](https://blog.evjang.com/2016/08/variational-bayes.html)).

<h2 id='summary'>Summaries</h2>

In this post, we begin with the concept of variational inference, then derive the ELBO from several points of view, and finally dig deeper facts behind the ELBO. As I mentioned at the very beginning, it plays an important role because it provides a framework in which the statistical inference can be transformed into optimization, leading to more and more amazing applications in the deep learning community.

Thanks for your interest and reading! 
