---
layout: post
title: ELBO — What & Why
---

ELBO (evidence lower bound) is a key quantity in [Variational Bayesian Methods](https://en.wikipedia.org/wiki/Variational_Bayesian_methods). It transforms inference problems, which are always _intractable_, into optimization problems that can be solved with, for example, gradient-based methods.

<details>
    <summary><h2>Updates</h2></summary>
    <b>&#8226; April 16, 2021</b>
    <p>An extensional derivation for the case of temporal sequences has been updated <a href='#temp_seq'>here</a>.
    </p>
    <b>&#8226; April 13, 2021</b>
    <p>Eq. (4) has been corrected in which the entropy term should be a conditional entropy of $Z$ given $X$, i.e., $H \left(Z \vert X \right)$, instead of $H \left(Z\right)$ in the previous version.</p>    
</details>



## Introduction

In this post, I'll introduce an important concept in  [Variational Bayesian (VB) Methods](https://en.wikipedia.org/wiki/Variational_Bayesian_methods) — ELBO (evidence lower bound, also known as variational lower bound) — and its derivations, alongside some digging into it. ELBO enables the rewriting of statistical inference problems as optimization problems, the so-called _inference-optimization duality_ (see [Eric Jang's blog](https://blog.evjang.com/2016/08/variational-bayes.html)). Combined with optimization methods such as gradient descent and modern approximation techniques, e.g., deep neural networks, inference on complex distributions can be achieved. Numerous applications can be found in [VAE](https://arxiv.org/abs/1312.6114v10), [DVRL](https://arxiv.org/abs/1806.02426), [MERLIN](https://arxiv.org/abs/1803.10760), to name a few.

## Table of Contents

* <a href='#motivation'>Motivation</a>
* <a href='#derivation_kl'>Derivation: From a View of KL Divergence</a>
* <a href='#derivation_logpx'>Derivation: From a View of Observations</a>
* <a href='#insight'>Insights</a>
* <a href='#temp_seq'>Extension: ELBO for Temporal Sequence</a>
* <a href='#summary'>Summaries</a>

<h2 id='motivation'>Motivation</h2>

Statistical inference problems focus on inferring the value of one random variable given the value of another random variable. Given the observation, $x$, we are interested in finding the _posterior_ distribution of the latent variable $z$ (we call it "latent" as it is not observed), $p(z \vert x)$. However, this is generally _intractable_ as when we express it using [Bayes' theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem), the computation of the marginal distribution of observation at the denominator requires the integration (or summation) over the whole latent space:

$$
\begin{align}
p(z|x)=\frac{p(z)p(x \vert z)}{p(x)}=\frac{p(z)p(x \vert z)}{\int_z p(x,z) dz}.
\tag{1}
\end{align}
$$

A natural question would be that "Can we use another distribution, let's say $q(z \vert x)$, as an approximation of the true _posterior_ distribution?" The answer is "Yes" and I shall explain in the next section.

<h2 id='derivation_kl'>Derivation: From a View of KL Divergence</h2>

Consider the problem that using a distribution $q_\theta(z \vert x)$ parameterized by $\theta$ to approximate the true _posterior_ distribution $p(z \vert x)$, the first issue we need to address is to select a suitable criterion to measure how close is $q_\theta(z \vert x)$ to $p(z \vert x)$. [Kullback–Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) is such a metric. Specifically, we use reverse KL divergence here (see [Eric Jang's blog](https://blog.evjang.com/2016/08/variational-bayes.html) for more insights into forward KL and reverse KL). 

The KL divergence between $q_\theta(z \vert x)$ and $p(z \vert x)$ is

$$
\begin{align}
D_{KL}\left(q_\theta(z \vert x) \Vert p(z \vert x)\right) &= \int_z q_\theta (z \vert x) \log \frac{q_\theta(z \vert x)}{p(z \vert x)} dz\\
&= -\int_z q_\theta(z \vert x) \log \frac{p(z \vert x)}{q_\theta(z \vert x)} dz\\
&= -\int_z q_\theta(z \vert x) \log \frac{p(z,x)}{q_\theta(z \vert x)p(x)} dz \\
&= - \left( \int_z q_\theta(z \vert x) \log \frac{p(z,x)}{q_\theta(z \vert x)} dz - \int_z q_\theta(z \vert x) \log p(x) dz\right) \\
&= - \int_z q_\theta(z \vert x) \log \frac{p(z,x)}{q_\theta(z \vert x)} dz + \log p(x) \\ 
&= \int_z q_\theta(z \vert x) \log \frac{q_\theta(z \vert x)}{p(z,x)} dz + \log p(x).
\tag{2}
\end{align}
$$

To minimize the KL divergence _w.r.t._ parameters $\theta$ we need to minimize the term $\int_z q_\theta(z \vert x) \log \frac{q_\theta(z \vert x)}{p(z,x)} dz$. It leads to maximize its negation $\mathcal{L} = -\int_z q_\theta(z \vert x) \log \frac{q_\theta(z \vert x)}{p(z,x)} dz$, which is known as the _evidence lower bound_ (ELBO). We will find some intuitive explanations about its name by plugging $\mathcal{L}$ into Eq. (2):

$$
\begin{align}
\log p(x) = \mathcal{L} + D_{KL}\left(q_\theta(z \vert x) \Vert p(z \vert x)\right).
\tag{3}
\end{align}
$$

By definition, the term "_evidence_" is the value of a likelihood function evaluated with fixed parameters. Note that the KL divergence at the right-hand side of Eq. (3) is non-negative, the term $\log p(x)$ is greater or equal to $\mathcal{L}$. And hence $\mathcal{L}$ sets a lower bound for evidence.

<h2 id='derivation_logpx'>Derivation: From a View of Observations</h2>

If we start the derivation from the marginal distribution of observations,

$$
\begin{align}
\log p(x) &= \log \int_z p(x, z) dz \\
&= \log \int_z p(x, z) \frac{q_\theta(z \vert x)}{q_\theta(z \vert x)} dz \\
&= \log \mathbb{E}_{z \sim q_\theta(z \vert x)} \left[ \frac{p(x, z)}{q_\theta(z \vert x)}\right] \\
&\geq \mathbb{E}_z \left[ \log \frac{p(x,z)}{q_\theta(z \vert x)}\right] \text{by Jensen's inequality} \\
&= \mathbb{E}_z \left[ \log p(x,z) \right] + \int_z q_\theta(z \vert x) \log \frac{1}{q_\theta(z \vert x)} dz \\
&= \mathbb{E}_z \left[ \log p(x,z) \right] + H(Z \vert X)
\tag{4},
\end{align}
$$

where $H(Z \vert X)$ is the conditional entropy. It is straightforward to show that the right hand of Eq. (4) is the same as $\mathcal{L}$, one expression of ELBO we derived in the previous section. It is also obvious that the term $\mathbb{E}_z \left[ \log p(x,z) \right] + H(Z \vert X)$ is a lower bound for the log-marginal probability of observations, which means that we just need to maximize ELBO when we wish to maximize the marginal probability of observations.

<h2 id='insight'>Insights</h2>

First of all, let's derive further from Eq. (4):

$$
\begin{align}
\mathcal{L} &= \mathbb{E}_{z \sim q_\theta(z \vert x)} \left[ \log p(x,z) \right] + H(Z \vert X) \\
&= \int_z q_\theta(z \vert x) \log \left(p(x|z)p(z)\right)dz + \int_z q_\theta(z \vert x) \log \frac{1}{q_\theta(z \vert x)} dz\\
&= \int_z q_\theta(z \vert x) \log p(x|z) dz + \int_z q_\theta(z \vert x) \log p(z) dz + \int_z q_\theta(z \vert x) \log \frac{1}{q_\theta(z \vert x)} dz\\
&= \mathbb{E}_z \left[ \log p(x|z)\right] - \int_z q_\theta(z \vert x) \log \left(\frac{q_\theta(z \vert x)}{p(z)}\right) dz \\
&= \mathbb{E}_z \left[ \log p(x|z)\right] - D_{KL}\left(q_\theta(z \vert x) \Vert p(z)\right).
\tag{5}
\end{align}
$$

Eq. (5) shows that the ELBO has two terms, one expected log-likelihood term and one KL divergence term. It suggests that the ELBO is a trade-off between the reconstruction accuracy against the complexity of the *posterior* approximation. The KL divergence term can be interpreted as a measure of the additional information required to express the *posterior* relative to the *prior*. As it approaches zero, the *posterior* is fully obtainable from the *prior*.

Furthermore, let's think about the reason behind the reverse KL divergence we used to derive Eq. (2):

$$
\begin{align}
D_{KL}\left(q_\theta(z \vert x) \Vert p(z \vert x) \right) = \int_z q_\theta(z \vert x) \log \frac{q_\theta(z \vert x)}{p(z \vert x)} dz.
\tag{6}
\end{align}
$$

It turns out that the variational distribution $q_\theta(z \vert x)$ is prevented from spanning the whole space. Consider the case where the denominator in Eq. (6) is zero, the value of $q_\theta(z \vert x)$ has to be zero as well otherwise the KL divergence goes to infinity. In summary, the reverse KL divergence has the effect of zero-forcing as minimizing it leads to $q_\theta (z \vert x)$ being squeezed under $p (z \vert x)$ (see [Eric Jang's blog](https://blog.evjang.com/2016/08/variational-bayes.html)).

<h2 id='temp_seq'>Extension: ELBO for Temporal Sequence</h2>

Consider the case that we wish to build a generative model $$p \left(\mathbf{x}_{0:t}, \mathbf{z}_{0:t} \right)$$ for sequential data $$\mathbf{x}_{0:t} \equiv \left(x_0, x_1, \ldots, x_t \right)$$ with a sequence of latent variable $$\mathbf{z}_{0:t} \equiv \left(z_0, z_1, \ldots, z_t \right)$$, we can also derive a corresponding ELBO as a surrogate objective. Optimizing this objective leads to the maximization of the likelihood of the sequential observations.

$$
\begin{align}
\log p \left(\mathbf{x}_{0:t} \right) &= \log \int_{\mathbf{z}_{0:t}} p \left(\mathbf{x}_{0:t}, \mathbf{z}_{0:t} \right) d\mathbf{z}_{0:t} \\
&= \log \int_{\mathbf{z}_{0:t}} p \left(\mathbf{x}_{0:t}, \mathbf{z}_{0:t} \right) \frac{q_\theta\left(\mathbf{z}_{0:t} \vert \mathbf{x}_{0:t}  \right)}{q_\theta\left(\mathbf{z}_{0:t} \vert \mathbf{x}_{0:t}  \right)}d\mathbf{z}_{0:t} \\
&= \log \mathbb{E}_{\mathbf{z}_{0:t} \sim q_\theta \left( \mathbf{z}_{0:t} \vert \mathbf{x}_{0:t}\right)} \left[ \frac{p \left(\mathbf{x}_{0:t}, \mathbf{z}_{0:t} \right) }{q_\theta \left( \mathbf{z}_{0:t} \vert \mathbf{x}_{0:t}\right)} \right] \\
&\geq \mathbb{E}_{\mathbf{z}_{0:t}} \left[\log \frac{p \left(\mathbf{x}_{0:t}, \mathbf{z}_{0:t} \right) }{q_\theta \left( \mathbf{z}_{0:t} \vert \mathbf{x}_{0:t}\right)}\right] \text{by Jensen's inequality}
\tag{7}
\end{align}
$$

So far, this is similar to what we have derived for the stationary case, i.e., Eq. (4) in the previous <a href='#derivation_logpx'>section</a>. However, the following derivation will require some factorizations of the joint distribution and the variational posterior. Concretely, we factorize the temporal model $$p \left(\mathbf{x}_{0:t}, \mathbf{z}_{0:t} \right)$$ and the approximation $$q_\theta \left( \mathbf{z}_{0:t} \vert \mathbf{x}_{0:t} \right)$$ as 

$$
p \left(\mathbf{x}_{0:t}, \mathbf{z}_{0:t} \right) = \prod_{\tau = 0}^t p \left(x_\tau \vert z_\tau\right) p \left(z_\tau \vert \mathbf{z}_{0:\tau -1}\right),
\tag{8}
$$

and

$$
q_\theta \left( \mathbf{z}_{0:t} \vert \mathbf{x}_{0:t} \right) = \prod_{\tau=0}^t q_\theta \left(z_{\tau} \vert \mathbf{z}_{0:\tau-1}, \mathbf{x}_{0:\tau}\right),
\tag{9}
$$

respectively.

To understand these factorizations, we can think that at each time step, the observation conditions on the latent variable at that time step, which also conditions on all latent variables before that time step. Expressing this relation recursively leads to Eq. (8). Similarly, the approximated latent variable at each time step conditions on the sequential observations up to that time and the history of latent variables, which is Eq. (9).

With these two factorizations, we can further derive Eq. (7) by plugging Eq. (8) and Eq. (9):

$$
\begin{align}
&\mathbb{E}_{\mathbf{z}_{0:t}} \left[\log \frac{p \left(\mathbf{x}_{0:t}, \mathbf{z}_{0:t} \right) }{q_\theta \left( \mathbf{z}_{0:t} \vert \mathbf{x}_{0:t}\right)}\right] \\
&= \mathbb{E}_{\mathbf{z}_{0:t}} \left[\log \frac{\prod_{\tau = 0}^t p \left(x_\tau \vert z_\tau\right) p \left(z_\tau \vert \mathbf{z}_{0:\tau -1}\right)}{\prod_{\tau=0}^t q_\theta \left(z_{\tau} \vert \mathbf{z}_{0:\tau-1}, \mathbf{x}_{0:\tau}\right)}\right] \\
&= \mathbb{E}_{\mathbf{z}_{0:t}} \left[\sum_{\tau=0}^t \log p \left(x_\tau \vert z_\tau\right) + \log p \left(z_\tau \vert \mathbf{z}_{0:\tau -1}\right) - \log q_\theta \left(z_{\tau} \vert \mathbf{z}_{0:\tau-1}, \mathbf{x}_{0:\tau}\right)  \right] \\
&= \sum_{\tau=0}^t \mathbb{E}_{\mathbf{z}_{0:t}} \left[\log p \left(x_\tau \vert z_\tau\right) + \log p \left(z_\tau \vert \mathbf{z}_{0:\tau -1}\right) - \log q_\theta \left(z_{\tau} \vert \mathbf{z}_{0:\tau-1}, \mathbf{x}_{0:\tau}\right) \right].
\end{align}
\tag{10}
$$

Now we will use one trick to replace variables. Note that as the variable $$\tau$$ starts from 0 to $$t$$, those items being taken expectation, i.e., $$\log p \left(x_\tau \vert z_\tau\right) + \log p \left(z_\tau \vert \mathbf{z}_{0:\tau -1}\right) - \log q_\theta \left(z_{\tau} \vert \mathbf{z}_{0:\tau-1}, \mathbf{x}_{0:\tau}\right)$$ will become invalid for $$\tau< \tau' \leq t$$. Therefore, we can write the original expectation term $$\mathbb{E}_{\mathbf{z}_{0:t}} [\cdot]$$ as $$\mathbb{E}_{\mathbf{z}_{0:\tau}} [\cdot]$$. Furthermore, another trick will allow us to factorize the expectation. Given the expectation taken _w.r.t._ $$\mathbf{z}_{0:\tau} \sim q_\theta \left(\mathbf{z}_{0:\tau} \vert \mathbf{x}_{0:\tau}\right)$$, i.e., $$\mathbb{E}_{\mathbf{z}_{0:\tau} \sim q_\theta \left(\mathbf{z}_{0:\tau} \vert \mathbf{x}_{0:\tau}\right)}[\cdot]$$, we can factorize it as $$\mathbb{E}_{z_\tau \sim q_\theta \left(z_\tau \vert \mathbf{z}_{0:\tau-1}, \mathbf{x}_{0:\tau}\right)} \mathbb{E}_{\mathbf{z}_{0:\tau-1} \sim q_\theta \left(\mathbf{z}_{0:\tau-1}\vert \mathbf{x}_{0:\tau-1}\right)}[\cdot]$$. With these tricks at hands, Eq. (10) can be written as

$$
\begin{align}
& \sum_{\tau=0}^t \mathbb{E}_{\mathbf{z}_{0:t}} \left[\log p \left(x_\tau \vert z_\tau\right) + \log p \left(z_\tau \vert \mathbf{z}_{0:\tau -1}\right) - \log q_\theta \left(z_{\tau} \vert \mathbf{z}_{0:\tau-1}, \mathbf{x}_{0:\tau}\right) \right] \\
&= \sum_{\tau=0}^t \mathbb{E}_{z_\tau}\mathbb{E}_{\mathbf{z}_{0:\tau - 1}} \left[\log p \left(x_\tau \vert z_\tau\right) + \log p \left(z_\tau \vert \mathbf{z}_{0:\tau -1}\right) - \log q_\theta \left(z_{\tau} \vert \mathbf{z}_{0:\tau-1}, \mathbf{x}_{0:\tau}\right) \right] \\
&= \sum_{\tau=0}^t \mathbb{E}_{\mathbf{z}_{0:\tau - 1}}\mathbb{E}_{z_\tau} \left[\log p \left(x_\tau \vert z_\tau\right) + \log p \left(z_\tau \vert \mathbf{z}_{0:\tau -1}\right) - \log q_\theta \left(z_{\tau} \vert \mathbf{z}_{0:\tau-1}, \mathbf{x}_{0:\tau}\right) \right] \\
&= \sum_{\tau=0}^t \mathbb{E}_{\mathbf{z}_{0:\tau - 1}}\mathbb{E}_{z_\tau} \left[\log p \left(x_\tau \vert z_\tau\right) - \log \frac{q_\theta \left(z_{\tau} \vert \mathbf{z}_{0:\tau-1}, \mathbf{x}_{0:\tau}\right)}{p \left(z_\tau \vert \mathbf{z}_{0:\tau -1}\right)} \right] \\
&= \sum_{\tau=0}^t \mathbb{E}_{\mathbf{z}_{0:\tau - 1}}\left[\mathbb{E}_{z_\tau} \left[\log p \left(x_\tau \vert z_\tau\right)\right] - \mathbb{E}_{z_\tau}\left[\log \frac{q_\theta \left(z_{\tau} \vert \mathbf{z}_{0:\tau-1}, \mathbf{x}_{0:\tau}\right)}{p \left(z_\tau \vert \mathbf{z}_{0:\tau -1}\right)} \right]\right] \\
&= \sum_{\tau=0}^t \mathbb{E}_{\mathbf{z}_{0:\tau - 1}}\left[\mathbb{E}_{z_\tau} \left[\log p \left(x_\tau \vert z_\tau\right)\right] - D_{KL}\left(q_\theta \left(z_{\tau} \vert \mathbf{z}_{0:\tau-1}, \mathbf{x}_{0:\tau}\right)\Vert p\left(z_\tau \vert \mathbf{z}_{0:\tau -1}\right)  \right)\right].
\end{align}
\tag{11}
$$

Put all of them together, we have derived a lower bound for the log-likelihood of temporal sequence. Great!

$$
\begin{align}
&\log p \left(\mathbf{x}_{0:t} \right) \geq \\
&\sum_{\tau=0}^t \mathbb{E}_{\mathbf{z}_{0:\tau - 1}}\left[\mathbb{E}_{z_\tau} \left[\log p \left(x_\tau \vert z_\tau\right)\right] - D_{KL}\left(q_\theta \left(z_{\tau} \vert \mathbf{z}_{0:\tau-1}, \mathbf{x}_{0:\tau}\right)\Vert p\left(z_\tau \vert \mathbf{z}_{0:\tau -1}\right)  \right)\right]
\end{align}
\tag{12}
$$

If we compare the derived ELBO for temporal sequence, i.e., Eq. (12), with the ELBO for the stationary observation, i.e., Eq. (5), we will find that ELBO for sequential observations is computed firstly by calculating the ELBO for a certain time step. Then this result is taken expectation _w.r.t._ histories of latent variables considering the property of a sequence. Finally, results are summed up along time step. Don't be scared by the math, it is fairly easy to understand if we start from the stationary case.

$$
\log p \left(\mathbf{x}_{0:t} \right) \geq \sum_{\tau=0}^t \mathbb{E}_{\mathbf{z}_{0:\tau - 1}}\left[  \underbrace{\mathbb{E}_{z_\tau} \left[\log p \left(x_\tau \vert z_\tau\right)\right] - D_{KL}\left(q_\theta \left(z_{\tau} \vert \mathbf{z}_{0:\tau-1}, \mathbf{x}_{0:\tau}\right)\Vert p\left(z_\tau \vert \mathbf{z}_{0:\tau -1}\right)  \right)}_{\text{Eq. (5)}} \right]
$$

<h2 id='summary'>Summaries</h2>

In this post, we begin with the concept of variational inference, then derive the ELBO from several points of view, and finally, dig deeper facts behind the ELBO. An extension of derivation for temporal sequences is also provided. As I mentioned at the very beginning, it plays an important role because it provides a framework in which the statistical inference can be transformed into optimization, leading to more and more amazing applications in the deep learning community.

Thanks for your interest and reading! 
